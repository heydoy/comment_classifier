{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "tf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "comment_classifier.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MBp6fXj_xfQ"
      },
      "source": [
        "# 1. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xxXj8xH_xfT"
      },
      "source": [
        "try:\n",
        "  # Colab only\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv4Y8nlJ_xfU"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKemDJGo_xfV"
      },
      "source": [
        "#colab에서 실행 중이라면...\n",
        "!git clone https://github.com/hukim1112/comment_classifier.git\n",
        "import os\n",
        "os.chdir('/content/comment_classifier')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPERT5Gr_xfV"
      },
      "source": [
        "import tensorflow as tf\n",
        "from konlpy.tag import Twitter\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "keras = tf.keras\n",
        "t = Twitter()\n",
        "t.morphs(\"나는 배고프다\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzZLNnee_xfW"
      },
      "source": [
        "# 2. fit tokenizer to our datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyCGQiB__xfW"
      },
      "source": [
        "from vectorizer import BaseVectorizer\n",
        "tokenizer = BaseVectorizer(t.morphs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqmKrRdq_xfW"
      },
      "source": [
        "df = pd.read_csv('train_intent.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5rmKH30_xfX"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG33GFj1H-M0"
      },
      "source": [
        "label_to_id = {t:i for i,t in enumerate(df.intent.unique())}\n",
        "id_to_label = {i:t for i,t in enumerate(df.intent.unique())}\n",
        "print(label_to_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqc4IbD6_xfX"
      },
      "source": [
        "tokenizer.fit(df['question'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-BMsccA_xfX"
      },
      "source": [
        "tokenizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RaY-kIL_xfX"
      },
      "source": [
        "# 3. data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emAGfx7-_xfZ"
      },
      "source": [
        "MAX_LENGTH = 40\n",
        "def tokenize_and_filter(sentences, labels):\n",
        "  inputs, outputs = [], []\n",
        "  \n",
        "  for sentence, label in zip(sentences, labels):\n",
        "    # tokenize sentence\n",
        "    tokenized_sentence = tokenizer.encode_a_doc_to_list(sentence)\n",
        "    # check tokenized sentence max length\n",
        "    if len(tokenized_sentence) <= MAX_LENGTH:\n",
        "      inputs.append(tokenized_sentence)\n",
        "      outputs.append(label_to_id[label])\n",
        "  \n",
        "  # pad tokenized sentences\n",
        "  padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      inputs, maxlen=MAX_LENGTH, padding='post', \n",
        "      value = tokenizer.vocabulary_['_PAD_']) # value = 0\n",
        "  \n",
        "  return padded_inputs, outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV47YqOW_xfa"
      },
      "source": [
        "inputs, outputs = tokenize_and_filter(df.question, df.intent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP5XQp-d_xfa"
      },
      "source": [
        "print('encoded input : ', inputs[0], 'label : ', outputs[0], 'original input sentence : ', tokenizer.decode_from_list(inputs[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agJ6trdu_xfb"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 7836\n",
        "\n",
        "# decoder inputs use the previous target as input\n",
        "# remove START_TOKEN from targets\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "dataset = dataset.shuffle(3000, reshuffle_each_iteration=False)\n",
        "val_ds = dataset.take(500)\n",
        "train_ds = dataset.skip(500)\n",
        "\n",
        "def configure_ds(dataset):\n",
        "  dataset = dataset.cache()\n",
        "  dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset\n",
        "train_ds = configure_ds(train_ds)\n",
        "val_ds = configure_ds(val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xEQl9P_c_xfc"
      },
      "source": [
        "for x, y in train_ds.take(1):\n",
        "    print(x, y)\n",
        "    print('-----------------------------------------------')\n",
        "    print(x.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM-jbs6U_xfc"
      },
      "source": [
        "# 4. model design"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stTwC73u_xfd"
      },
      "source": [
        "num_class = len(label_to_id.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_1oMbSL_xfd"
      },
      "source": [
        "def get_model():\n",
        "    return tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(tokenizer.n_vocabs, 64),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_class, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj7zsReW_xfe"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTGqJJ_-_xfe"
      },
      "source": [
        "LEARNING_RATE = 0.0001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTHLXxDT_xff"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43P6DrB__xff"
      },
      "source": [
        "model.fit(train_ds, epochs=10, validation_data=val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvUyT88v_xfg"
      },
      "source": [
        "def question_processing(sentences):\n",
        "    inputs = []\n",
        "    for sentence in sentences:\n",
        "        # tokenize sentence\n",
        "        tokenized_sentence = tokenizer.encode_a_doc_to_list(sentence)\n",
        "        # check tokenized sentence max length\n",
        "        if len(tokenized_sentence) <= MAX_LENGTH:\n",
        "            inputs.append(tokenized_sentence)\n",
        "        else:\n",
        "            print('입력이 너무 길어요.')\n",
        "    # pad tokenized sentences\n",
        "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    inputs, maxlen=MAX_LENGTH, padding='post', \n",
        "    value = tokenizer.vocabulary_['_PAD_']) # value = 0\n",
        "    return padded_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5IRG-OJFh-d"
      },
      "source": [
        "question_processing([\"나는 전주 날씨 궁금함\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xwnyLs2_xfh"
      },
      "source": [
        "input_sentence = question_processing(['서울 날씨 어때?', \n",
        "                                      '나는 전주 날씨 궁금함',\n",
        "                                      '안중근 의사는 누구야?',\n",
        "                                      '명동 맛있는 음식점 있니?'\n",
        "                                     ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKu5rRiP_xfj"
      },
      "source": [
        "model.predict(input_sentence) #클래스 별 확률 예측"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTVpCK7D_xfj"
      },
      "source": [
        "prediction = np.argmax(model.predict(input_sentence), axis=1)\n",
        "print(prediction) #예측한 클래스의 ID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU_UOA2n_xfk"
      },
      "source": [
        "'''\n",
        "  '서울 날씨 어때?', \n",
        "  '나는 전주 날씨 궁금함',\n",
        "  '안중근 의사는 누구야?',\n",
        "  '명동 맛있는 음식점 있니?'\n",
        "\n",
        "'''\n",
        "for p in prediction:\n",
        "    print(id_to_label[p])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAa9A4x2_xfk"
      },
      "source": [
        "del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONSsJmI1_xfk"
      },
      "source": [
        "# 데이터 추가해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvw3JHDm_xfl"
      },
      "source": [
        "names = ['안중근', '이순신', '세종대왕', '김광석', '아이유', '에미넴', '이건희', '고아라', '유재석', '한석희', '최민성']\n",
        "def question_generator(names):\n",
        "    question = []\n",
        "    for name in names:\n",
        "        s1 = name+'는 어떤 분이야?'\n",
        "        s2 = name+'은 어떤 사람이니?'\n",
        "        s3 = name+'이란 사람에 대해 궁금해'\n",
        "        question = question+[s1, s2, s3]\n",
        "    return question\n",
        "question = question_generator(names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj1qWVhA_xfl"
      },
      "source": [
        "question"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VpIQNst_xfl"
      },
      "source": [
        "new_data = {'question' : question, 'intent' : ['인물']*len(question)}\n",
        "add_df = pd.DataFrame(new_data, columns=('question', 'intent'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y10ZT0Ai_xfm"
      },
      "source": [
        "add_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaalL8Ln_xfm"
      },
      "source": [
        "print(len(df), len(add_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kcy6R8h_xfm"
      },
      "source": [
        "new_df = pd.concat([df, add_df])\n",
        "print(len(new_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDQ7l4by_xfn"
      },
      "source": [
        "tokenizer.fit(new_df['question'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHHqOYPM_xfn"
      },
      "source": [
        "new_inputs, new_outputs = tokenize_and_filter(new_df.question, new_df.intent)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 7836\n",
        "\n",
        "# decoder inputs use the previous target as input\n",
        "# remove START_TOKEN from targets\n",
        "dataset = tf.data.Dataset.from_tensor_slices((new_inputs, new_outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiYVOPc5_xfo"
      },
      "source": [
        "new_model = get_model()\n",
        "LEARNING_RATE = 0.0001\n",
        "new_model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
        "new_model.fit(dataset, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J88KUxZs_xfo"
      },
      "source": [
        "input_sentence = question_processing(['서울 날씨 어때?', \n",
        "                                      '나는 전주 날씨 궁금함',\n",
        "                                      '안중근 의사는 누구야?',\n",
        "                                      '박소희는 어떤 사람인지 궁금해.',\n",
        "                                      '명동 맛있는 음식점 있니?'\n",
        "                                     ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wyDdvy__xfp"
      },
      "source": [
        "new_model.predict(input_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYQnW7Bo_xfp"
      },
      "source": [
        "prediction = np.argmax(new_model.predict(input_sentence), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T61tc3Sk_xfq"
      },
      "source": [
        "for p in prediction:\n",
        "    print(id_to_label[p])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fDrKGu__xfr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}